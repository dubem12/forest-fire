{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import dtreeviz\n",
    "import graphviz\n",
    "import graphviz.backend as be\n",
    "from IPython.display import Image, display_svg, SVG\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", module=\"matplotlib\\..*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUSINESS QUESTIONS\n",
    "\n",
    "\n",
    "*The major concern is the role of seasons in ISI rating. \n",
    "The common questions include, what season do most fires take place?\n",
    "As fire prevention also might require the presence of active workforce for early detection, the absence of a workforce due to weekends was also evaluated. As we don´t have full control of the data details, it was hard to factor in public holidays and the normal work schedule of forest managers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "import matplotlib.font_manager\n",
    "fm = matplotlib.font_manager\n",
    "fm._get_fontconfig_fonts.cache_clear()\n",
    "plt.rcParams['font.family'] = 'Times New Roman'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"forestfireexp.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop month and day\n",
    "df=df.drop(['month', 'day'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## making sense of the data\n",
    "\n",
    "The data focus is on the ISI which correlates to the velocity of fire spread. It´s related to the Forest Fire Weather Index which was a system developed in canada for rating the threat of forest fires , and has subsequently been proven to fit in the Mediterrenian forests (Cortez and Morias \"A Data Mining Approach to Predict Forest Fires using Meteorological Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1\n",
    "# What role does season play in causing fires in public lands\n",
    "#using ISI as a measure of fire danger rating, we can see the role of season in causing fires in public lands\n",
    "#title of graph is \"Fire Danger Rating by Season\"\n",
    "\n",
    "df['season'].value_counts().plot(kind='pie')\n",
    "plt.title('Fire Recording by Season')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check ISI aavergae by season , assign different color shades of blue to each season\n",
    "df.groupby('season')['ISI'].mean().plot(kind='bar', color=['orange', 'red', 'brown', 'teal'])\n",
    "plt.title('ISI by Season')\n",
    "plt.ylabel('ISI Rating')\n",
    "plt.xlabel('Season')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if ISI avergae per season is significantly different\n",
    "from scipy import stats\n",
    "spring = df[df['season']=='spring']['ISI']\n",
    "summer = df[df['season']=='summer']['ISI']\n",
    "autumn = df[df['season']=='autumn']['ISI']\n",
    "winter = df[df['season']=='winter']['ISI']\n",
    "stats.f_oneway(spring, summer, autumn, winter)\n",
    "\n",
    "f_statistic, p_value = stats.f_oneway(spring, summer, autumn, winter)\n",
    "\n",
    "print(\"F-statistic:\", f_statistic)\n",
    "print(\"p-value:\", p_value)\n",
    "\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"The differences in ISI averages per season are statistically significant, so we accept that seasons play a significant role in forest fire.\")\n",
    "else:\n",
    "    print(\n",
    "        \"The differences in ISI averages per season are not statistically significant.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check week day and weekend \n",
    "\n",
    "df.groupby(\"day_category\")[\"ISI\"].mean().plot(kind=\"bar\")\n",
    "plt.title(\"ISI Average by weekend or weekday\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if ISI avergae per day_category is significantly different\n",
    "weekday = df[df['day_category']=='weekday']['ISI']\n",
    "weekend = df[df['day_category']=='weekend']['ISI']\n",
    "stats.f_oneway(weekday, weekend)\n",
    "\n",
    "f_statistic, p_value = stats.f_oneway(weekday, weekend)\n",
    "\n",
    "print(\"F-statistic:\", f_statistic)\n",
    "print(\"p-value:\", p_value)\n",
    "\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\n",
    "        \"The differences in ISI averages per weekday or weekend are statistically significant, so we accept that the day of the week play a significant role in forest fire.\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"The differences in ISI averages per weekday or weekend are not statistically significant.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check area average by day_category\n",
    "colors = [\"blue\", \"orange\"]\n",
    "df.groupby(\"day_category\")[\"area\"].mean().plot(kind=\"bar\", color=colors)\n",
    "plt.title(\"Area Average by weekend or weekday\")\n",
    "plt.ylabel(\"Area Average\")  \n",
    "plt.xlabel(\"Day Category\")    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see the fire covers more area during weekends , which means we would need to consider rapid emergency response for weekends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check if the area average per day_category is significantly different\n",
    "weekday = df[df['day_category']=='weekday']['area']\n",
    "weekend = df[df['day_category']=='weekend']['area']\n",
    "stats.f_oneway(weekday, weekend)\n",
    "\n",
    "f_statistic, p_value = stats.f_oneway(weekday, weekend)\n",
    "\n",
    "print(\"F-statistic:\", f_statistic)\n",
    "print(\"p-value:\", p_value)\n",
    "\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\n",
    "        \"The differences in burnt areas averages per weekday or weekend are statistically significant, so we accept that workdays play a significant role in forest fire.\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"The differences in burnt areas averages per weekday or weekend are not statistically significant.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA MODELLING\n",
    "\n",
    "now to data modelling to predict using decision tree\n",
    "\n",
    "First we would try to predict the ISI under all conditions , secondly we will attempt to predict total affected area "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import dtreeviz\n",
    "import graphviz\n",
    "import graphviz.backend as be\n",
    "from IPython.display import Image, display_svg, SVG\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", module=\"matplotlib\\..*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "import matplotlib.font_manager\n",
    "fm = matplotlib.font_manager\n",
    "fm._get_fontconfig_fonts.cache_clear()\n",
    "plt.rcParams['font.family'] = 'Times New Roman'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df.drop(columns=['ISI'])\n",
    "y=df['ISI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2=x.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assing 1 to weekend and 0 to weekday, 1 to summer , 2 to autumn, 3 to winter and 4 to spring\n",
    "x2['day_category']=x2['day_category'].map({'weekday':0, 'weekend':1})\n",
    "x2['season']=x2['season'].map({'summer':1, 'autumn':2, 'winter':3, 'spring':4})\n",
    "\n",
    "x2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sunique value in season\n",
    "x2['season'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for skewness\n",
    "x2.skew()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we see skewness in FFMC , DC, rain , area and season. \n",
    "we will transform those coliumns to correct skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target ffmc, dc, rain , are and season are skewed, we will use power transformer to transform the data\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "pt=PowerTransformer()\n",
    "x2[['FFMC', 'DC', 'rain', 'area']]=pt.fit_transform(x2[['FFMC', 'DC', 'rain', 'area']])\n",
    "x2.skew()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check skewness\n",
    "x2.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check multicollinearity\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(x2.corr(), annot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#minmax scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler=MinMaxScaler()\n",
    "X=scaler.fit_transform(x2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to dataframe\n",
    "X=pd.DataFrame(X, columns=x2.columns)\n",
    "X.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split X and y into training and testing sets\n",
    "X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#view the shape of the training and testing sets\n",
    "X_train_df = pd.DataFrame(X_train, columns=x2.columns)\n",
    "X_test_df = pd.DataFrame(X_test, columns=x2.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#search for a great parameter with GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "max_depth = [3, 6]\n",
    "criterion_choice = ['squared_error', 'absolute_error']\n",
    "min_samples_split = [2, 10] \n",
    "min_samples_leaf = [2, 5] \n",
    "\n",
    "param_grid = {'max_depth': max_depth, 'criterion': criterion_choice, 'min_samples_split': min_samples_split, 'min_samples_leaf': min_samples_leaf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeRegressor()\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the model\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best r2\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best parameters\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the best parameters to fit the model\n",
    "model = DecisionTreeRegressor(criterion='squared_error', max_depth=6, min_samples_leaf=5, min_samples_split=3)\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REJECT LOW MODEL SCORE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USE RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "max_depth_choices = np.random.randint(\n",
    "    low=1, high=len(X.columns), size=3\n",
    ")  # A random integer between 1 and the number of columns\n",
    "criterion_choices = [\n",
    "    \"squared_error\",\n",
    "    \"absolute_error\",\n",
    "]  # A list of the possible values optimization metrics\n",
    "min_samples_split_choices = np.random.randint(\n",
    "    low=2, high=10, size=3\n",
    ")  # A random integer between 1 and the number of columns\n",
    "min_samples_leaf_choices = np.random.randint(\n",
    "    low=2, high=10, size=3\n",
    ")  # A random integer between 1 and the number of columns\n",
    "max_features_choices = np.random.randint(\n",
    "    low=1, high=len(X.columns), size=3\n",
    ")  # A random integer between 1 and the number of columns\n",
    "\n",
    "random_grid = {\n",
    "    \"max_depth\": max_depth_choices,\n",
    "    \"criterion\": criterion_choices,\n",
    "    \"min_samples_split\": min_samples_split_choices,\n",
    "    \"min_samples_leaf\": min_samples_leaf_choices,\n",
    "    \"max_features\": max_features_choices,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeRegressor()\n",
    "# n_iter is how many random combinations of hyperparameters will test use the computer.\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=model, param_distributions=random_grid, n_iter=25, cv=5, n_jobs=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use random forest for model2\n",
    "\n",
    "model2 = DecisionTreeRegressor(min_samples_split= 7, min_samples_leaf = 9, max_features = 8, max_depth = 6, criterion = 'squared_error')\n",
    "\n",
    "model2.fit(X_train, y_train)\n",
    "\n",
    "model2.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = [round(value, 2) for value in y_train_pred]\n",
    "y_test_pred = [round(value, 2) for value in y_test_pred]\n",
    "\n",
    "# Create a dictionary with the results\n",
    "results = {\n",
    "    \"Set\": [\"Train\"] * len(X_train) + [\"Test\"] * len(X_test),\n",
    "    \"Real\": list(y_train) + list(y_test),\n",
    "    \"Predicted\": y_train_pred + y_test_pred,\n",
    "}\n",
    "\n",
    "# Create the results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check most important features that predicts ISI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = model.feature_importances_\n",
    "feature_importance = pd.DataFrame({\"feature\": X.columns, \"importance\": importances})\n",
    "feature= feature_importance.sort_values(by='importance', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert featre importance column to percentage\n",
    "feature2=feature.copy()\n",
    "feature2['importance']=feature2['importance']*100\n",
    "\n",
    "feature2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display error margin\n",
    "results_df[\"Errors\"] = results_df[\"Real\"] - results_df[\"Predicted\"]\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select errors where errors is not equal to zero\n",
    "errors = results_df[results_df[\"Errors\"] != 0]\n",
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize real and predicted values\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.scatterplot(data=results_df, x=\"Real\", y=\"Predicted\", hue=\"Set\")\n",
    "sns.lineplot(data=results_df, x=\"Real\", y=\"Real\", color=\"black\")\n",
    "plt.title(\"Real vs Predicted Values\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check model performance\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Mean Squared Error\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "# Mean Absolute Error\n",
    "mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "# R2 Score\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "\n",
    "def error_metrics_report(\n",
    "    y_real_train: list, y_real_test: list, y_pred_train: list, y_pred_test: list\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    The error metrics report function calculates for regression.\n",
    "\n",
    "     Parameters:\n",
    "     - y_real_train (list): The actual target values for the training dataset.\n",
    "     - y_real_test (list): The actual target values for the testing dataset.\n",
    "     - y_pred_train (list): The predicted target values for the training dataset.\n",
    "     - y_pred_test (list): The predicted target values for the testing dataset.\n",
    "\n",
    "     Returns:\n",
    "     - metrics_df (DataFrame): A Pandas DataFrame containing error metrics for both the training and testing datasets.\n",
    "     - 'Metric' (str): The name of the error metric.\n",
    "     - 'Training Set' (float): The error metric value for the training set.\n",
    "     - 'Testing Set' (float): The error metric value for the testing set.\n",
    "    \"\"\"\n",
    "\n",
    "    MAE_train = mean_absolute_error(y_real_train, y_pred_train)\n",
    "    MAE_test = mean_absolute_error(y_real_test, y_pred_test)\n",
    "\n",
    "    # Mean squared error\n",
    "    MSE_train = mean_squared_error(y_real_train, y_pred_train)\n",
    "    MSE_test = mean_squared_error(y_real_test, y_pred_test)\n",
    "\n",
    "    # Root mean squared error\n",
    "    RMSE_train = mean_squared_error(y_real_train, y_pred_train)\n",
    "    RMSE_test = mean_squared_error(y_real_test, y_pred_test)\n",
    "\n",
    "    # R2\n",
    "    R2_train = r2_score(y_real_train, y_pred_train)\n",
    "    R2_test = r2_score(y_real_test, y_pred_test)\n",
    "\n",
    "    results = {\n",
    "        \"Metric\": [\"MAE\", \"MSE\", \"RMSE\", \"R2\"],\n",
    "        \"Train\": [MAE_train, MSE_train, RMSE_train, R2_train],\n",
    "        \"Test\": [MAE_test, MSE_test, RMSE_test, R2_test],\n",
    "    }\n",
    "\n",
    "    results_df = pd.DataFrame(results).round(2)\n",
    "\n",
    "    pd.set_option(\"display.float_format\", lambda x: \"{:.2f}\".format(x))\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_metrics_report(\n",
    "    list(results_df[results_df[\"Set\"] == \"Train\"][\"Real\"]),\n",
    "    list(results_df[results_df[\"Set\"] == \"Test\"][\"Real\"]),\n",
    "    list(results_df[results_df[\"Set\"] == \"Train\"][\"Predicted\"]),\n",
    "    list(results_df[results_df[\"Set\"] == \"Test\"][\"Predicted\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R2 score on the test set was very poor. Subsequent projects will seek to establish a better model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
